{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Common functions\n",
        "Contains common functions needed for the script snippets below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when, to_date, avg, to_timestamp, col, cast\n",
        "from pyspark.sql.dataframe import DataFrame\n",
        "\n",
        "storageAccountName = \"dynamicsstagingsa.dfs.core.windows.net\"\n",
        "containerName = \"stagingdata\"\n",
        "manifestPath = \"%s/sample/CDS/model.json\" % (containerName)\n",
        "outputPath = \"output\"\n",
        "\n",
        "def readEntityFromLake(storageAccount, manifest, entityName):\n",
        "    dataFrame = (spark.read.format(\"com.microsoft.cdm\")\n",
        "        .option(\"storage\", storageAccount)\n",
        "        .option(\"manifestPath\", manifest)\n",
        "        .option(\"entity\", entityName)\n",
        "        .option(\"mode\", \"permissive\")\n",
        "        .load())\n",
        "\n",
        "    return dataFrame\n",
        "\n",
        "def writeToCsv(dataFrame: DataFrame, csvName):\n",
        "    csvPath = 'abfss://%s@%s/%s/%s' % (containerName, storageAccountName, outputPath, csvName)\n",
        "    dataFrame.write.csv(csvPath, mode = 'overwrite', header = 'true')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Cases per day\n",
        "This calculates the number of cases per day, by queue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "queueDf = readEntityFromLake(storageAccountName, manifestPath, \"queue\")\n",
        "queueItemsDf = readEntityFromLake(storageAccountName, manifestPath, \"queueitem\")\n",
        "incidentDf = readEntityFromLake(storageAccountName, manifestPath, \"incident\")\n",
        "\n",
        "# Define columns to select\n",
        "incidentColumns = [\"createdon\", \"incidentid\", \"title\"]\n",
        "queueItemsColumns = [\"queueid\", \"objectid\"]\n",
        "queueColumns = [\"queueid\", \"name\"]\n",
        "joinedDfColumns = [\"incidentid\", \"createdon\", \"name\"]\n",
        "\n",
        "# Filter data frames for required rows and columns\n",
        "filteredIncidentDf = incidentDf \\\n",
        "                        .filter(incidentDf.createdon.isNotNull()) \\\n",
        "                        .select(*incidentColumns)\n",
        "\n",
        "filteredQueueItemDf = queueItemsDf \\\n",
        "                        .filter(queueItemsDf.queueid.isNotNull() & queueItemsDf.objectid.isNotNull()) \\\n",
        "                        .select(*queueItemsColumns) \\\n",
        "                        .withColumnRenamed(\"queueid\", \"qi_queueid\")\n",
        "\n",
        "filteredQueueDf = queueDf \\\n",
        "                    .withColumn(\"name\", when((queueDf.name.isNull()) | (queueDf.name == \"\"), \"<Unnamed Queue>\")\n",
        "                                        .otherwise(queueDf.name)) \\\n",
        "                    .select(*queueColumns)\n",
        "\n",
        "# Join the data sets\n",
        "joinedDf = filteredQueueItemDf \\\n",
        "                .join(filteredQueueDf, filteredQueueItemDf.qi_queueid == filteredQueueDf.queueid, \"inner\") \\\n",
        "                .join(filteredIncidentDf, filteredQueueItemDf.objectid == filteredIncidentDf.incidentid, \"rightouter\") \\\n",
        "                .select(*joinedDfColumns)\n",
        "\n",
        "joinedDf = joinedDf \\\n",
        "                .withColumn(\"name\", when(joinedDf.name.isNull(), \"<No_Queue_Assigned>\")\n",
        "                                    .otherwise(joinedDf.name)) \\\n",
        "                .withColumn(\"createdon_date\", to_date(joinedDf.createdon))\n",
        "\n",
        "# Group joined data set on created date and queue name\n",
        "groupedDf = joinedDf \\\n",
        "                .groupBy(joinedDf.createdon_date, joinedDf.name) \\\n",
        "                .count() \\\n",
        "                .orderBy(joinedDf.createdon_date, joinedDf.name) \\\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\") \\\n",
        "                .withColumnRenamed(\"name\", \"queue_name\")\n",
        "\n",
        "groupedDf.show(truncate=False)\n",
        "writeToCsv(groupedDf, \"IncidentsPerDay\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Participants by session\n",
        "This calculates the number of participants by session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sessionParticipantDf = readEntityFromLake(storageAccountName, manifestPath, \"msdyn_sessionparticipant\")\n",
        "\n",
        "# Define columns to select\n",
        "sessionParticipantColumns = [\"createdon\",\"msdyn_omnichannelsession\", \"msdyn_omnichannelsessionname\", \"msdyn_sessionparticipantid\"]\n",
        "\n",
        "# Filter data frames for required rows and columns\n",
        "filteredSessionDf = sessionParticipantDf \\\n",
        "                        .filter(sessionParticipantDf.createdon.isNotNull()) \\\n",
        "                        .select(*sessionParticipantColumns) \\\n",
        "                        .withColumn(\"createdon_date\", to_date(sessionParticipantDf.createdon))\n",
        "\n",
        "# Group joined data set on created date and queue name\n",
        "groupedDf = filteredSessionDf \\\n",
        "                .groupBy(filteredSessionDf.createdon_date,filteredSessionDf.msdyn_omnichannelsession, filteredSessionDf.msdyn_omnichannelsessionname) \\\n",
        "                .count() \\\n",
        "                .orderBy(filteredSessionDf.createdon_date, filteredSessionDf.msdyn_omnichannelsession) \\\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\")\n",
        "\n",
        "groupedDf.show(truncate=False)\n",
        "writeToCsv(groupedDf, \"ParticipantsPerSession\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Average Scheduled duration, actual duration and on hold time for sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sessionDf = readEntityFromLake(storageAccountName, manifestPath, \"msdyn_ocsession\")\n",
        "\n",
        "# Define columns to select\n",
        "sessionColumns = [\"createdon\",\"msdyn_sessionid\", \"actualdurationminutes\", \"scheduleddurationminutes\",\"onholdtime\"]\n",
        "\n",
        "# Filter data frames for required rows and columns\n",
        "filteredSessionDf = sessionDf \\\n",
        "                        .filter(sessionDf.createdon.isNotNull()) \\\n",
        "                        .select(*sessionColumns) \\\n",
        "                        .withColumn(\"createdon_date\", to_date(sessionDf.createdon))\n",
        "\n",
        "# Group joined data set on created date and queue name\n",
        "groupedDf = filteredSessionDf.groupBy(filteredSessionDf.createdon_date) \\\n",
        "                .agg(avg(filteredSessionDf.actualdurationminutes),avg(filteredSessionDf.scheduleddurationminutes),avg(filteredSessionDf.onholdtime)) \\\n",
        "                .orderBy(filteredSessionDf.createdon_date) \\\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\")\n",
        "\n",
        "groupedDf.show(truncate=False)\n",
        "writeToCsv(groupedDf, \"SessionMetrics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Time to assign live work item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "workItemDf = readEntityFromLake(storageAccountName, manifestPath, \"msdyn_ocliveworkitem\")\n",
        "\n",
        "# Define columns to select\n",
        "workItemColumns = [\"createdon\",\"msdyn_activeagentassignedon\"]\n",
        "\n",
        "# Filter data frames for required rows and columns\n",
        "filteredDf = workItemDf \\\n",
        "                        .filter(workItemDf.createdon.isNotNull()) \\\n",
        "                        .select(*workItemColumns) \\\n",
        "                        .withColumn(\"createdon_date\", to_date(workItemDf.createdon)) \\\n",
        "                        .withColumn(\"createdon_timestamp\", to_timestamp(workItemDf.createdon)) \\\n",
        "                        .withColumn(\"assignedon_timestamp\", to_timestamp(workItemDf.msdyn_activeagentassignedon)) \\\n",
        "                        .withColumn(\"timetoassigninseconds\", col(\"assignedon_timestamp\").cast(\"long\") - col(\"createdon_timestamp\").cast(\"long\"))\n",
        "\n",
        "\n",
        "# Group joined data set on created date and queue name\n",
        "groupedDf = filteredDf.groupBy(filteredDf.createdon_date) \\\n",
        "                .agg(avg(filteredDf.timetoassigninseconds)) \\\n",
        "                .orderBy(filteredDf.createdon_date) \\\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\")\n",
        "\n",
        "groupedDf.show(truncate=False)\n",
        "writeToCsv(groupedDf, \"TimeToAssignWorkItem\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Average wrap up time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "workItemDf = readEntityFromLake(storageAccountName, manifestPath, \"msdyn_ocliveworkitem\")\n",
        "# Define columns to select\n",
        "workItemColumns = [\"createdon\",\"msdyn_activeagentassignedon\",\"msdyn_wrapupinitiatedon\",\"actualend\"]\n",
        "# Filter data frames for required rows and columns\n",
        "filteredDf = workItemDf \\\n",
        "                        .filter(workItemDf.createdon.isNotNull()) \\\n",
        "                        .select(*workItemColumns) \\\n",
        "                        .withColumn(\"createdon_date\", to_date(workItemDf.createdon)) \\\n",
        "                        .withColumn(\"wrapupstart_timestamp\", to_timestamp(workItemDf.msdyn_wrapupinitiatedon)) \\\n",
        "                        .withColumn(\"end_timestamp\", to_timestamp(workItemDf.actualend)) \\\n",
        "                        .withColumn(\"timetowrapinseconds\", col(\"actualend\").cast(\"long\") - col(\"msdyn_wrapupinitiatedon\").cast(\"long\"))\n",
        "\n",
        "# Group joined data set on created date and queue name\n",
        "groupedDf = filteredDf.groupBy(filteredDf.createdon_date) \\\n",
        "                .agg(avg(filteredDf.timetowrapinseconds)) \\\n",
        "                .orderBy(filteredDf.createdon_date) \\\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\")\n",
        "groupedDf.show(truncate=False)\n",
        "writeToCsv(groupedDf, \"WrapUpTime\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Average idle time for agent in a session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sesionParticipantDf = readEntityFromLake(storageAccountName, manifestPath, \"msdyn_sessionparticipant\")\n",
        "# Define columns to select\n",
        "workItemColumns = [\"createdon\",\"msdyn_idletime\",\"msdyn_agentidname\"]\n",
        "# Filter data frames for required rows and columns\n",
        "filteredSessionDf = sesionParticipantDf \\\n",
        "                        .filter(sesionParticipantDf.createdon.isNotNull()) \\\n",
        "                        .select(*workItemColumns) \\\n",
        "                        .withColumn(\"createdon_date\", to_date(sesionParticipantDf.createdon)) \\\n",
        "# Group joined data set on created date and queue name\n",
        "groupedDf = filteredSessionDf.groupBy(filteredSessionDf.createdon_date,filteredSessionDf.msdyn_agentidname) \\\n",
        "                .agg(avg(filteredSessionDf.msdyn_idletime)) \\\n",
        "                .orderBy(filteredSessionDf.createdon_date) \\\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\")\n",
        "groupedDf.show(truncate=False)\n",
        "writeToCsv(groupedDf, \"IdleTimeSession\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Average session duration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sessionDf = readEntityFromLake(storageAccountName, manifestPath, \"msdyn_ocsession\")\n",
        "# Define columns to select\n",
        "workItemColumns = [\"createdon\",\"actualstart\",\"actualend\"]\n",
        "# Filter data frames for required rows and columns\n",
        "filteredDf = sessionDf \\\n",
        "                        .filter(sessionDf.createdon.isNotNull()) \\\n",
        "                        .select(*workItemColumns) \\\n",
        "                        .withColumn(\"createdon_date\", to_date(sessionDf.createdon)) \\\n",
        "                        .withColumn(\"start_timestamp\", to_timestamp(sessionDf.actualstart)) \\\n",
        "                        .withColumn(\"end_timestamp\", to_timestamp(sessionDf.actualend)) \\\n",
        "                        .withColumn(\"timeinseconds\", col(\"actualend\").cast(\"long\") - col(\"start_timestamp\").cast(\"long\"))\n",
        "\n",
        "# Group joined data set on created date and queue name\n",
        "groupedDf = filteredDf.groupBy(filteredDf.createdon_date) \\\n",
        "                .agg(avg(filteredDf.timeinseconds)) \\\n",
        "                .orderBy(filteredDf.createdon_date) \\\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\")\n",
        "groupedDf.show(truncate=False)\n",
        "writeToCsv(groupedDf, \"SessionDuration\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
