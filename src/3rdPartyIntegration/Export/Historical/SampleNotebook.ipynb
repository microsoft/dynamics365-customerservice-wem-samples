{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Common functions\r\n",
        "Contains common functions needed for the script snippets below"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, to_date, avg, to_timestamp, col, cast\r\n",
        "from pyspark.sql.dataframe import DataFrame\r\n",
        "\r\n",
        "storageAccountName = \"dynamicsstagingsa.dfs.core.windows.net\"\r\n",
        "containerName = \"stagingdata\"\r\n",
        "manifestPath = \"%s/sample/CDS/model.json\" % (containerName)\r\n",
        "outputPath = \"output\"\r\n",
        "\r\n",
        "def readEntityFromLake(storageAccount, manifest, entityName):\r\n",
        "    dataFrame = (spark.read.format(\"com.microsoft.cdm\")\r\n",
        "        .option(\"storage\", storageAccount)\r\n",
        "        .option(\"manifestPath\", manifest)\r\n",
        "        .option(\"entity\", entityName)\r\n",
        "        .option(\"mode\", \"permissive\")\r\n",
        "        .load())\r\n",
        "\r\n",
        "    return dataFrame\r\n",
        "\r\n",
        "def writeToCsv(dataFrame: DataFrame, csvName):\r\n",
        "    csvPath = 'abfss://%s@%s/%s/%s' % (containerName, storageAccountName, outputPath, csvName)\r\n",
        "    dataFrame.write.csv(csvPath, mode = 'overwrite', header = 'true')\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": "4",
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-11T17:07:11.1241131Z",
              "session_start_time": "2023-07-11T17:07:11.1801498Z",
              "execution_start_time": "2023-07-11T17:10:08.3900046Z",
              "execution_finish_time": "2023-07-11T17:10:08.5656429Z",
              "spark_jobs": null,
              "parent_msg_id": "b8fddd5e-83ea-4f46-b2cc-3271ea8178f1"
            },
            "text/plain": "StatementMeta(SparkPool01, 4, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cases per day\n",
        "This calculates the number of cases per day, by queue."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queueDf = readEntityFromLake(storageAccountName, manifestPath, \"queue\")\r\n",
        "queueItemsDf = readEntityFromLake(storageAccountName, manifestPath, \"queueitem\")\r\n",
        "incidentDf = readEntityFromLake(storageAccountName, manifestPath, \"incident\")\r\n",
        "\r\n",
        "# Define columns to select\r\n",
        "incidentColumns = [\"createdon\", \"incidentid\", \"title\"]\r\n",
        "queueItemsColumns = [\"queueid\", \"objectid\"]\r\n",
        "queueColumns = [\"queueid\", \"name\"]\r\n",
        "joinedDfColumns = [\"incidentid\", \"createdon\", \"name\"]\r\n",
        "\r\n",
        "# Filter data frames for required rows and columns\r\n",
        "filteredIncidentDf = incidentDf \\\r\n",
        "                        .filter(incidentDf.createdon.isNotNull()) \\\r\n",
        "                        .select(*incidentColumns)\r\n",
        "\r\n",
        "filteredQueueItemDf = queueItemsDf \\\r\n",
        "                        .filter(queueItemsDf.queueid.isNotNull() & queueItemsDf.objectid.isNotNull()) \\\r\n",
        "                        .select(*queueItemsColumns) \\\r\n",
        "                        .withColumnRenamed(\"queueid\", \"qi_queueid\")\r\n",
        "\r\n",
        "filteredQueueDf = queueDf \\\r\n",
        "                    .withColumn(\"name\", when((queueDf.name.isNull()) | (queueDf.name == \"\"), \"<Unnamed Queue>\")\r\n",
        "                                        .otherwise(queueDf.name)) \\\r\n",
        "                    .select(*queueColumns)\r\n",
        "\r\n",
        "# Join the data sets\r\n",
        "joinedDf = filteredQueueItemDf \\\r\n",
        "                .join(filteredQueueDf, filteredQueueItemDf.qi_queueid == filteredQueueDf.queueid, \"inner\") \\\r\n",
        "                .join(filteredIncidentDf, filteredQueueItemDf.objectid == filteredIncidentDf.incidentid, \"rightouter\") \\\r\n",
        "                .select(*joinedDfColumns)\r\n",
        "\r\n",
        "joinedDf = joinedDf \\\r\n",
        "                .withColumn(\"name\", when(joinedDf.name.isNull(), \"<No_Queue_Assigned>\")\r\n",
        "                                    .otherwise(joinedDf.name)) \\\r\n",
        "                .withColumn(\"createdon_date\", to_date(joinedDf.createdon))\r\n",
        "\r\n",
        "# Group joined data set on created date and queue name\r\n",
        "groupedDf = joinedDf \\\r\n",
        "                .groupBy(joinedDf.createdon_date, joinedDf.name) \\\r\n",
        "                .count() \\\r\n",
        "                .orderBy(joinedDf.createdon_date, joinedDf.name) \\\r\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\") \\\r\n",
        "                .withColumnRenamed(\"name\", \"queue_name\")\r\n",
        "\r\n",
        "groupedDf.show(truncate=False)\r\n",
        "writeToCsv(groupedDf, \"IncidentsPerDay\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": "4",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-11T17:10:33.5377516Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-11T17:10:33.6884669Z",
              "execution_finish_time": "2023-07-11T17:11:15.9224886Z",
              "spark_jobs": null,
              "parent_msg_id": "6dc3912b-abbf-4b4b-b420-4af4f72d566d"
            },
            "text/plain": "StatementMeta(SparkPool01, 4, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------------+-----+\n|date      |queue_name            |count|\n+----------+----------------------+-----+\n|2021-12-23|<No_Queue_Assigned>   |1    |\n|2022-11-01|Forecast case queue 01|81   |\n|2022-11-01|Forecast case queue 02|109  |\n|2022-11-02|Forecast case queue 01|94   |\n|2022-11-02|Forecast case queue 02|127  |\n|2022-11-03|Forecast case queue 01|90   |\n|2022-11-03|Forecast case queue 02|121  |\n|2022-11-04|Forecast case queue 01|90   |\n|2022-11-04|Forecast case queue 02|120  |\n|2022-11-05|Forecast case queue 01|93   |\n|2022-11-05|Forecast case queue 02|127  |\n|2022-11-07|Forecast case queue 01|71   |\n|2022-11-07|Forecast case queue 02|90   |\n|2022-11-08|Forecast case queue 01|99   |\n|2022-11-08|Forecast case queue 02|120  |\n|2022-11-09|Forecast case queue 01|100  |\n|2022-11-09|Forecast case queue 02|120  |\n|2022-11-10|Forecast case queue 01|100  |\n|2022-11-10|Forecast case queue 02|120  |\n|2022-11-11|Forecast case queue 01|100  |\n+----------+----------------------+-----+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Participants by session\r\n",
        "This calculates the number of participants by session"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sessionParticipantDf = readEntityFromLake(storageAccountName, manifestPath, \"msdyn_sessionparticipant\")\r\n",
        "\r\n",
        "# Define columns to select\r\n",
        "sessionParticipantColumns = [\"createdon\",\"msdyn_omnichannelsession\", \"msdyn_omnichannelsessionname\", \"msdyn_sessionparticipantid\"]\r\n",
        "\r\n",
        "# Filter data frames for required rows and columns\r\n",
        "filteredSessionDf = sessionParticipantDf \\\r\n",
        "                        .filter(sessionParticipantDf.createdon.isNotNull()) \\\r\n",
        "                        .select(*sessionParticipantColumns) \\\r\n",
        "                        .withColumn(\"createdon_date\", to_date(sessionParticipantDf.createdon))\r\n",
        "\r\n",
        "# Group joined data set on created date and queue name\r\n",
        "groupedDf = filteredSessionDf \\\r\n",
        "                .groupBy(filteredSessionDf.createdon_date,filteredSessionDf.msdyn_omnichannelsession, filteredSessionDf.msdyn_omnichannelsessionname) \\\r\n",
        "                .count() \\\r\n",
        "                .orderBy(filteredSessionDf.createdon_date, filteredSessionDf.msdyn_omnichannelsession) \\\r\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\")\r\n",
        "\r\n",
        "groupedDf.show(truncate=False)\r\n",
        "writeToCsv(groupedDf, \"ParticipantsPerSession\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": "4",
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-11T17:11:23.8600098Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-11T17:11:24.0079802Z",
              "execution_finish_time": "2023-07-11T17:11:34.8386682Z",
              "spark_jobs": null,
              "parent_msg_id": "65f08233-261c-4313-b955-1a956eaa602d"
            },
            "text/plain": "StatementMeta(SparkPool01, 4, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------------------------+---------------------------------------------+-----+\n|date      |msdyn_omnichannelsession            |msdyn_omnichannelsessionname                 |count|\n+----------+------------------------------------+---------------------------------------------+-----+\n|2023-03-07|000f4df8-42bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat 41457b2e5a|1    |\n|2023-03-07|003ae761-42bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat 235c2b277b|1    |\n|2023-03-07|00be383a-43bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat 5c3a78eea2|1    |\n|2023-03-07|010a26f8-41bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat 45c83e1289|1    |\n|2023-03-07|01332b70-43bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat af0a336cce|1    |\n|2023-03-07|013a35b5-41bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat d7bb1c339f|1    |\n|2023-03-07|014f3034-43bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat 4e6ea179fd|1    |\n|2023-03-07|0157fb13-44bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat 7dca493dfd|1    |\n|2023-03-07|01653aaf-41bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat 922a0e70ac|1    |\n|2023-03-07|01cf3b04-43bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat c1ccceb513|1    |\n|2023-03-07|01d0cb91-42bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat e7da3213e9|1    |\n|2023-03-07|01f7f71f-44bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat 3bc53935e1|1    |\n|2023-03-07|02ad37bb-41bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat d6a74c27f6|1    |\n|2023-03-07|02cb052a-42bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat 80112d2f4e|1    |\n|2023-03-07|035ce5c1-42bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat c8ed59166f|1    |\n|2023-03-07|03a4c297-42bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat 3b0cbf75cb|1    |\n|2023-03-07|03b149e2-43bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat 7db9993ec8|1    |\n|2023-03-07|03d50030-42bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat 09d0cffa92|1    |\n|2023-03-07|03d643fe-42bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat 67d71b9bd2|1    |\n|2023-03-07|03e3fa19-44bd-ed11-b594-000d3a5d2dc9|Session for Forecast demodata chat bbd08add35|1    |\n+----------+------------------------------------+---------------------------------------------+-----+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Average Scheduled duration, actual duration and on hold time for sessions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sessionDf = readEntityFromLake(storageAccountName, manifestPath, \"msdyn_ocsession\")\r\n",
        "\r\n",
        "# Define columns to select\r\n",
        "sessionColumns = [\"createdon\",\"msdyn_sessionid\", \"actualdurationminutes\", \"scheduleddurationminutes\",\"onholdtime\"]\r\n",
        "\r\n",
        "# Filter data frames for required rows and columns\r\n",
        "filteredSessionDf = sessionDf \\\r\n",
        "                        .filter(sessionDf.createdon.isNotNull()) \\\r\n",
        "                        .select(*sessionColumns) \\\r\n",
        "                        .withColumn(\"createdon_date\", to_date(sessionDf.createdon))\r\n",
        "\r\n",
        "# Group joined data set on created date and queue name\r\n",
        "groupedDf = filteredSessionDf.groupBy(filteredSessionDf.createdon_date) \\\r\n",
        "                .agg(avg(filteredSessionDf.actualdurationminutes),avg(filteredSessionDf.scheduleddurationminutes),avg(filteredSessionDf.onholdtime)) \\\r\n",
        "                .orderBy(filteredSessionDf.createdon_date) \\\r\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\")\r\n",
        "\r\n",
        "groupedDf.show(truncate=False)\r\n",
        "writeToCsv(groupedDf, \"SessionMetrics\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": "4",
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-11T17:11:32.1529772Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-11T17:11:35.0041214Z",
              "execution_finish_time": "2023-07-11T17:11:45.7852516Z",
              "spark_jobs": null,
              "parent_msg_id": "4c4195fa-e37d-42fd-95b8-0ddfc1daea1c"
            },
            "text/plain": "StatementMeta(SparkPool01, 4, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------------+-----------------------------+---------------+\n|date      |avg(actualdurationminutes)|avg(scheduleddurationminutes)|avg(onholdtime)|\n+----------+--------------------------+-----------------------------+---------------+\n|2022-12-01|null                      |null                         |null           |\n|2022-12-02|null                      |null                         |null           |\n|2022-12-03|null                      |null                         |null           |\n|2022-12-04|null                      |null                         |null           |\n|2022-12-05|null                      |null                         |null           |\n|2022-12-06|null                      |null                         |null           |\n|2022-12-07|null                      |null                         |null           |\n|2022-12-08|null                      |null                         |null           |\n|2022-12-09|null                      |null                         |null           |\n|2022-12-10|null                      |null                         |null           |\n|2022-12-11|null                      |null                         |null           |\n|2022-12-12|null                      |null                         |null           |\n|2022-12-13|null                      |null                         |null           |\n|2022-12-14|null                      |null                         |null           |\n|2022-12-15|null                      |null                         |null           |\n|2022-12-16|null                      |null                         |null           |\n|2022-12-17|null                      |null                         |null           |\n|2022-12-18|null                      |null                         |null           |\n|2022-12-19|null                      |null                         |null           |\n|2022-12-20|null                      |null                         |null           |\n+----------+--------------------------+-----------------------------+---------------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time to assign live work item"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workItemDf = readEntityFromLake(storageAccountName, manifestPath, \"msdyn_ocliveworkitem\")\r\n",
        "\r\n",
        "# Define columns to select\r\n",
        "workItemColumns = [\"createdon\",\"msdyn_activeagentassignedon\"]\r\n",
        "\r\n",
        "# Filter data frames for required rows and columns\r\n",
        "filteredDf = workItemDf \\\r\n",
        "                        .filter(workItemDf.createdon.isNotNull()) \\\r\n",
        "                        .select(*workItemColumns) \\\r\n",
        "                        .withColumn(\"createdon_date\", to_date(workItemDf.createdon)) \\\r\n",
        "                        .withColumn(\"createdon_timestamp\", to_timestamp(workItemDf.createdon)) \\\r\n",
        "                        .withColumn(\"assignedon_timestamp\", to_timestamp(workItemDf.msdyn_activeagentassignedon)) \\\r\n",
        "                        .withColumn(\"timetoassigninseconds\", col(\"assignedon_timestamp\").cast(\"long\") - col(\"createdon_timestamp\").cast(\"long\"))\r\n",
        "\r\n",
        "\r\n",
        "# Group joined data set on created date and queue name\r\n",
        "groupedDf = filteredDf.groupBy(filteredDf.createdon_date) \\\r\n",
        "                .agg(avg(filteredDf.timetoassigninseconds)) \\\r\n",
        "                .orderBy(filteredDf.createdon_date) \\\r\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\")\r\n",
        "\r\n",
        "groupedDf.show(truncate=False)\r\n",
        "writeToCsv(groupedDf, \"TimeToAssignWorkItem\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": "4",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-11T17:11:43.451686Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-11T17:11:45.9502115Z",
              "execution_finish_time": "2023-07-11T17:11:56.869475Z",
              "spark_jobs": null,
              "parent_msg_id": "727b3099-7276-480e-a1d1-49b145e8ab6b"
            },
            "text/plain": "StatementMeta(SparkPool01, 4, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------------+\n|date      |avg(timetoassigninseconds)|\n+----------+--------------------------+\n|2022-12-01|3.887218045112782         |\n|2022-12-02|3.849462365591398         |\n|2022-12-03|3.972375690607735         |\n|2022-12-04|4.049180327868853         |\n|2022-12-05|4.0701754385964914        |\n|2022-12-06|4.046632124352332         |\n|2022-12-07|3.9244186046511627        |\n|2022-12-08|3.7559808612440193        |\n|2022-12-09|4.048648648648649         |\n|2022-12-10|4.052910052910053         |\n|2022-12-11|3.955223880597015         |\n|2022-12-12|4.037593984962406         |\n|2022-12-13|3.9887640449438204        |\n|2022-12-14|4.043478260869565         |\n|2022-12-15|4.031413612565445         |\n|2022-12-16|3.8944723618090453        |\n|2022-12-17|4.0046728971962615        |\n|2022-12-18|3.8688524590163933        |\n|2022-12-19|3.852713178294574         |\n|2022-12-20|3.9408602150537635        |\n+----------+--------------------------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Average wrap up time"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workItemDf = readEntityFromLake(storageAccountName, manifestPath, \"msdyn_ocliveworkitem\")\r\n",
        "# Define columns to select\r\n",
        "workItemColumns = [\"createdon\",\"msdyn_activeagentassignedon\",\"msdyn_wrapupinitiatedon\",\"actualend\"]\r\n",
        "# Filter data frames for required rows and columns\r\n",
        "filteredDf = workItemDf \\\r\n",
        "                        .filter(workItemDf.createdon.isNotNull()) \\\r\n",
        "                        .select(*workItemColumns) \\\r\n",
        "                        .withColumn(\"createdon_date\", to_date(workItemDf.createdon)) \\\r\n",
        "                        .withColumn(\"wrapupstart_timestamp\", to_timestamp(workItemDf.msdyn_wrapupinitiatedon)) \\\r\n",
        "                        .withColumn(\"end_timestamp\", to_timestamp(workItemDf.actualend)) \\\r\n",
        "                        .withColumn(\"timetowrapinseconds\", col(\"actualend\").cast(\"long\") - col(\"msdyn_wrapupinitiatedon\").cast(\"long\"))\r\n",
        "\r\n",
        "# Group joined data set on created date and queue name\r\n",
        "groupedDf = filteredDf.groupBy(filteredDf.createdon_date) \\\r\n",
        "                .agg(avg(filteredDf.timetowrapinseconds)) \\\r\n",
        "                .orderBy(filteredDf.createdon_date) \\\r\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\")\r\n",
        "groupedDf.show(truncate=False)\r\n",
        "writeToCsv(groupedDf, \"WrapUpTime\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": "4",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-11T17:11:56.1895229Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-11T17:11:57.0865841Z",
              "execution_finish_time": "2023-07-11T17:12:07.8716374Z",
              "spark_jobs": null,
              "parent_msg_id": "86a45d77-2518-4515-b728-5a9444b427c4"
            },
            "text/plain": "StatementMeta(SparkPool01, 4, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------------+\n|date      |avg(timetowrapinseconds)|\n+----------+------------------------+\n|2022-12-01|74.48872180451127       |\n|2022-12-02|75.18279569892474       |\n|2022-12-03|74.41988950276243       |\n|2022-12-04|76.63934426229508       |\n|2022-12-05|74.25438596491227       |\n|2022-12-06|73.93782383419689       |\n|2022-12-07|74.27325581395348       |\n|2022-12-08|75.49282296650718       |\n|2022-12-09|74.63783783783784       |\n|2022-12-10|75.56613756613757       |\n|2022-12-11|76.08955223880596       |\n|2022-12-12|75.25563909774436       |\n|2022-12-13|75.80337078651685       |\n|2022-12-14|74.45108695652173       |\n|2022-12-15|75.31937172774869       |\n|2022-12-16|75.04020100502512       |\n|2022-12-17|74.8644859813084        |\n|2022-12-18|74.73770491803279       |\n|2022-12-19|75.11627906976744       |\n|2022-12-20|73.95161290322581       |\n+----------+------------------------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Average idle time for agent in a session"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sesionParticipantDf = readEntityFromLake(storageAccountName, manifestPath, \"msdyn_sessionparticipant\")\r\n",
        "# Define columns to select\r\n",
        "workItemColumns = [\"createdon\",\"msdyn_idletime\",\"msdyn_agentidname\"]\r\n",
        "# Filter data frames for required rows and columns\r\n",
        "filteredSessionDf = sesionParticipantDf \\\r\n",
        "                        .filter(sesionParticipantDf.createdon.isNotNull()) \\\r\n",
        "                        .select(*workItemColumns) \\\r\n",
        "                        .withColumn(\"createdon_date\", to_date(sesionParticipantDf.createdon)) \\\r\n",
        "# Group joined data set on created date and queue name\r\n",
        "groupedDf = filteredSessionDf.groupBy(filteredSessionDf.createdon_date,filteredSessionDf.msdyn_agentidname) \\\r\n",
        "                .agg(avg(filteredSessionDf.msdyn_idletime)) \\\r\n",
        "                .orderBy(filteredSessionDf.createdon_date) \\\r\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\")\r\n",
        "groupedDf.show(truncate=False)\r\n",
        "writeToCsv(groupedDf, \"IdleTimeSession\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": "4",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-11T17:12:05.2702323Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-11T17:12:08.0285319Z",
              "execution_finish_time": "2023-07-11T17:12:18.8621725Z",
              "spark_jobs": null,
              "parent_msg_id": "bc1b139c-f041-43eb-a20c-28353de86974"
            },
            "text/plain": "StatementMeta(SparkPool01, 4, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------+-------------------+\n|date      |msdyn_agentidname|avg(msdyn_idletime)|\n+----------+-----------------+-------------------+\n|2023-03-07|aurora user13    |0.0                |\n|2023-03-07|aurora user20    |0.0                |\n|2023-03-07|aurora user05    |0.0                |\n|2023-03-07|aurora user08    |0.0                |\n|2023-03-07|aurora user06    |0.0                |\n|2023-03-07|aurora user07    |0.0                |\n|2023-03-07|aurora user02    |0.0                |\n|2023-03-07|aurora user11    |0.0                |\n|2023-03-07|aurora user10    |0.0                |\n|2023-03-07|aurora user09    |0.0                |\n|2023-03-07|aurora user18    |0.0                |\n|2023-03-07|aurora user14    |0.0                |\n|2023-03-07|aurora user03    |0.0                |\n|2023-03-07|aurora user17    |0.0                |\n|2023-03-07|aurora user16    |0.0                |\n|2023-03-07|aurora user04    |0.0                |\n|2023-03-07|aurora user19    |0.0                |\n|2023-03-07|aurora user12    |0.0                |\n|2023-03-07|aurora user15    |0.0                |\n|2023-03-08|aurora user10    |0.0                |\n+----------+-----------------+-------------------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Average session duration"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sessionDf = readEntityFromLake(storageAccountName, manifestPath, \"msdyn_ocsession\")\r\n",
        "# Define columns to select\r\n",
        "workItemColumns = [\"createdon\",\"actualstart\",\"actualend\"]\r\n",
        "# Filter data frames for required rows and columns\r\n",
        "filteredDf = sessionDf \\\r\n",
        "                        .filter(sessionDf.createdon.isNotNull()) \\\r\n",
        "                        .select(*workItemColumns) \\\r\n",
        "                        .withColumn(\"createdon_date\", to_date(sessionDf.createdon)) \\\r\n",
        "                        .withColumn(\"start_timestamp\", to_timestamp(sessionDf.actualstart)) \\\r\n",
        "                        .withColumn(\"end_timestamp\", to_timestamp(sessionDf.actualend)) \\\r\n",
        "                        .withColumn(\"timeinseconds\", col(\"actualend\").cast(\"long\") - col(\"start_timestamp\").cast(\"long\"))\r\n",
        "\r\n",
        "# Group joined data set on created date and queue name\r\n",
        "groupedDf = filteredDf.groupBy(filteredDf.createdon_date) \\\r\n",
        "                .agg(avg(filteredDf.timeinseconds)) \\\r\n",
        "                .orderBy(filteredDf.createdon_date) \\\r\n",
        "                .withColumnRenamed(\"createdon_date\", \"date\")\r\n",
        "groupedDf.show(truncate=False)\r\n",
        "writeToCsv(groupedDf, \"SessionDuration\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": "4",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-07-11T17:12:29.7024434Z",
              "session_start_time": null,
              "execution_start_time": "2023-07-11T17:12:29.8455127Z",
              "execution_finish_time": "2023-07-11T17:12:40.7115356Z",
              "spark_jobs": null,
              "parent_msg_id": "de8aedb8-3af0-4671-8d10-1992b61f4a04"
            },
            "text/plain": "StatementMeta(SparkPool01, 4, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+\n|date      |avg(timeinseconds)|\n+----------+------------------+\n|2022-12-01|null              |\n|2022-12-02|null              |\n|2022-12-03|null              |\n|2022-12-04|null              |\n|2022-12-05|null              |\n|2022-12-06|null              |\n|2022-12-07|null              |\n|2022-12-08|null              |\n|2022-12-09|null              |\n|2022-12-10|null              |\n|2022-12-11|null              |\n|2022-12-12|null              |\n|2022-12-13|null              |\n|2022-12-14|null              |\n|2022-12-15|null              |\n|2022-12-16|null              |\n|2022-12-17|null              |\n|2022-12-18|null              |\n|2022-12-19|null              |\n|2022-12-20|null              |\n+----------+------------------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}